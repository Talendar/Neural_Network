{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlCWvUQS4h1d",
        "colab_type": "text"
      },
      "source": [
        "**< NEURAL NETWORK >**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j5iwiId4mDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    By Talendar (Gabriel Nogueira)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers_size=None, cost_type=\"mse\", layers_activation=\"sigmoid\", logs_path=\"./logs\"):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        :param layers_size: list containing the sizes of the layers. The layers activation functions will be the default one.\n",
        "        \"\"\"\n",
        "        self.cost_type = cost_type\n",
        "        self.layers = []\n",
        "        self.logs_path = logs_path\n",
        "\n",
        "        if layers_size is not None:\n",
        "            self.layers.append(NeuralLayer(layers_size[0], input_count=0, activation=\"input_layer\"))\n",
        "            for s in layers_size[1:]:\n",
        "                input_count = self.layers[-1].size\n",
        "                self.layers.append(NeuralLayer(s, input_count, layers_activation))\n",
        "\n",
        "\n",
        "    def add_layer(self, size, activation=\"sigmoid\"):\n",
        "        \"\"\"\n",
        "        :param size:\n",
        "        :param activation:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.layers.append(NeuralLayer(size[0], input_count=0, activation=\"input_layer\"))\n",
        "            for s in size[1:]:\n",
        "                input_count = self.layers[-1].size\n",
        "                self.layers.append(NeuralLayer(s, input_count, activation))\n",
        "\n",
        "        except TypeError:\n",
        "            if len(self.layers) == 0:\n",
        "                self.layers.append(NeuralLayer(size, input_count=0, activation=\"input_layer\"))\n",
        "            else:\n",
        "                input_count = self.layers[-1].size\n",
        "                self.layers.append(NeuralLayer(size, input_count, activation))\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Wrapper for the feedforward function that uses the network's current weights and bias.\n",
        "\n",
        "        :param x: column vector containing the features of the sample. If an out of shape numpy array is fed, this function\n",
        "        won't work properly due to errors in matrix multiplications.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        weights = [l.weights for l in self.layers[1:]]\n",
        "        bias = [l.bias for l in self.layers[1:]]\n",
        "\n",
        "        return self.feedforward(weights, bias, x)\n",
        "\n",
        "\n",
        "    def feedforward(self, weights, bias, x):\n",
        "        \"\"\"\n",
        "        Feedforward.\n",
        "\n",
        "        :param weights: list with the weights matrix for each layer (excluding the first layer).\n",
        "        :param bias: list with the bias vector for each layer (excluding the first layer).\n",
        "        :param x: column vector containing the features of the sample. If an out of shape numpy array is fed, this function\n",
        "        won't work properly due to errors in matrix multiplications.\n",
        "        :return: a vector (numpy array) containing the output of each neuron of the output layer.\n",
        "        \"\"\"\n",
        "        a = self.colvector(x)\n",
        "        for i, l in enumerate(self.layers[1:]):\n",
        "            w, b = weights[i], bias[i]\n",
        "            a = l.activate(np.dot(w, a) + b)\n",
        "\n",
        "        return a\n",
        "\n",
        "\n",
        "    def costfunc_unit(self, h, y, derivative=False):\n",
        "\n",
        "        \"\"\"\n",
        "        :param x:\n",
        "        :param y:\n",
        "        :param derivative:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if self.cost_type.lower() == \"mse\":\n",
        "            if not derivative:\n",
        "                return ((h - y) ** 2) / 2\n",
        "            else:\n",
        "                return h - y\n",
        "\n",
        "        raise NameError(\"Cost function of the type \\\"%s\\\" is not defined!\" % str(self.cost_type))\n",
        "\n",
        "\n",
        "    def costfunc(self, data):\n",
        "        \"\"\"\n",
        "        :param training_set:\n",
        "        :param labels:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        p_cols = 1\n",
        "        try:\n",
        "            p_cols = len(data[0][0])\n",
        "        except TypeError:\n",
        "            pass\n",
        "\n",
        "        l_cols = 1\n",
        "        try:\n",
        "            l_cols = len(data[0][1])\n",
        "        except TypeError:\n",
        "            pass\n",
        "\n",
        "        m = len(data)\n",
        "        predictions = np.zeros((m, p_cols))\n",
        "        labels = np.zeros((m, l_cols))\n",
        "\n",
        "        for i in range(m):\n",
        "            h, y = data[i]\n",
        "            predictions[i] = h.transpose()\n",
        "            labels[i] = y.transpose()\n",
        "\n",
        "        if self.cost_type.lower() == \"mse\":\n",
        "            loss = np.sum((predictions - labels) ** 2) / (2 * m)\n",
        "            return loss\n",
        "\n",
        "        raise NameError(\"Cost function of the type \\\"%s\\\" is not defined!\" % str(self.cost_type))\n",
        "\n",
        "\n",
        "    def colvector(self, v):\n",
        "        \"\"\"\n",
        "        Turn a numpy array into a column vector.\n",
        "        :param v:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        try:\n",
        "            v.shape = (len(v), 1)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        return v\n",
        "\n",
        "\n",
        "    def generate_mini_batches(self, training_data, labels, mini_batch_size):\n",
        "        \"\"\"\n",
        "        :param training_data:\n",
        "        :param mini_batch_size_pc:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        m = len(training_data)\n",
        "        indexes = [n for n in range(m)]\n",
        "        random.shuffle(indexes)\n",
        "\n",
        "        mini_batches_x = [training_data[indexes[k:(k + mini_batch_size)]] for k in range(0, m, mini_batch_size)]\n",
        "        mini_batches_y = [labels[indexes[k:(k + mini_batch_size)]] for k in range(0, m, mini_batch_size)]\n",
        "\n",
        "        return mini_batches_x, mini_batches_y\n",
        "\n",
        "\n",
        "    def backpropagation(self, x, y):\n",
        "        \"\"\"\n",
        "        :param x:\n",
        "        :param y:\n",
        "        :return: a tuple containing, respectively: the gradient of the cost function with respect to the weights; the\n",
        "        gradient of the cost function with respect to the bias; the prediction (activation result of the output layer)\n",
        "        of the model for the given sample.\n",
        "        \"\"\"\n",
        "        grad_w = [np.zeros(l.weights.shape) for l in self.layers[1:]]\n",
        "        grad_b = [np.zeros(l.bias.shape) for l in self.layers[1:]]\n",
        "\n",
        "        # FORWARD PASS\n",
        "        activations = [self.colvector(x)]\n",
        "        zs = []  # note that the input layer is not considered! So this list will have a size num_layers - 1\n",
        "\n",
        "        for l in self.layers[1:]:  # excluding the input layer from the iteration\n",
        "            w, b = l.weights, l.bias\n",
        "            a = self.colvector(activations[-1])\n",
        "            z = np.dot(w, a) + b\n",
        "\n",
        "            zs.append(z)\n",
        "            activations.append(l.activate(z))\n",
        "\n",
        "        # BACKWARD PASS\n",
        "        delta = self.costfunc_unit(activations[-1], y, derivative=True) * self.layers[-1].activate(zs[-1], derivative=True)  # initial delta is delta_L (delta for the output layer)\n",
        "\n",
        "        # gradients with respect to the output layer\n",
        "        grad_b[-1] = delta\n",
        "        grad_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        # gradients with respect to the hidden layers\n",
        "        for l in range((len(self.layers) - 2), 0, -1):\n",
        "            w_lp1, delta_lp1 = self.layers[l + 1].weights, delta\n",
        "            a_der = self.layers[l].activate(zs[l - 1], derivative=True)  # l - 1 because zs doesnt have the input layer!\n",
        "\n",
        "            delta = np.dot(w_lp1.transpose(), delta) * a_der\n",
        "            grad_b[l - 1] = delta\n",
        "            grad_w[l - 1] = np.dot(delta, activations[l - 1].transpose())\n",
        "\n",
        "        return grad_w, grad_b, activations[-1]\n",
        "\n",
        "\n",
        "    def params_to_row(self, weights, bias):\n",
        "        \"\"\"\n",
        "\n",
        "        :param weights:\n",
        "        :param bias:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        flat_params = np.concatenate((weights[0].flatten(), bias[0].flatten()))\n",
        "        for w, b in zip(weights[1:], bias[1:]):\n",
        "            temp = np.concatenate((w.flatten(), b.flatten()))\n",
        "            flat_params = np.concatenate((flat_params, temp))\n",
        "\n",
        "        return flat_params\n",
        "\n",
        "\n",
        "    def params_to_matrix(self, flat_params):\n",
        "        \"\"\"\n",
        "\n",
        "        :param flat_params:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        weights, bias = [], []\n",
        "        consumed = 0\n",
        "\n",
        "        for l in self.layers[1:]:\n",
        "            rows_w, cols_w = l.weights.shape\n",
        "            total_w = rows_w * cols_w\n",
        "            w = flat_params[consumed : consumed + total_w]\n",
        "            w.shape = (rows_w, cols_w)\n",
        "            weights.append(w)\n",
        "            consumed += total_w\n",
        "\n",
        "            rows_b, cols_b = l.bias.shape\n",
        "            total_b = rows_b * cols_b\n",
        "            b = flat_params[consumed: consumed + total_b]\n",
        "            b.shape = (rows_b, cols_b)\n",
        "            bias.append(b)\n",
        "            consumed += total_b\n",
        "\n",
        "        return weights, bias\n",
        "\n",
        "\n",
        "    def compute_gradient_numerically(self, data_x, data_y):\n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function numerically. To be used in gradient checking only.\n",
        "\n",
        "        :param data_x:\n",
        "        :param data_y:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        e = 1e-4\n",
        "        flat_params = self.params_to_row( [l.weights for l in self.layers[1:]] , [l.bias for l in self.layers[1:]] )\n",
        "        num_grad = np.zeros(flat_params.shape)\n",
        "        pertub = np.zeros(flat_params.shape)\n",
        "\n",
        "        for i in range(0, len(flat_params)):\n",
        "            pertub[i] = e\n",
        "            inc_params = self.params_to_matrix(flat_params + pertub)\n",
        "            dec_params = self.params_to_matrix(flat_params - pertub)\n",
        "\n",
        "            inc_predictions = [(self.feedforward(*inc_params, x), y) for x, y in zip(data_x, data_y)]\n",
        "            dec_predictions = [(self.feedforward(*dec_params, x), y) for x, y in zip(data_x, data_y)]\n",
        "\n",
        "            J = self.costfunc\n",
        "            num_grad[i] += (J(inc_predictions) - J(dec_predictions)) / (2*e)\n",
        "            pertub[i] = 0\n",
        "\n",
        "        return num_grad\n",
        "\n",
        "\n",
        "    def gradient_checking(self, bp_grad_w, bp_grad_b, data_x, data_y):\n",
        "        \"\"\"\n",
        "\n",
        "        :param bp_grad_w:\n",
        "        :param bp_grad_b:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        bp_grad = self.params_to_row(bp_grad_w, bp_grad_b)\n",
        "        num_grad = self.compute_gradient_numerically(data_x, data_y)\n",
        "\n",
        "        relative_error = np.linalg.norm(bp_grad - num_grad) / np.linalg.norm(bp_grad + num_grad)\n",
        "\n",
        "        with open(self.logs_path + \"/log_gradient_checking.txt\", \"a\") as file:\n",
        "            file.write(\"Relative error: %.4e\\n\\n\" % relative_error)\n",
        "\n",
        "\n",
        "\n",
        "    def regularization_term(self, m, method, factor, derivative=False, w=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param m:\n",
        "        :param method:\n",
        "        :param factor:\n",
        "        :param derivative:\n",
        "        :param w:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if method.lower() == \"l2\":\n",
        "            if derivative:\n",
        "                return w * factor / m\n",
        "            else:\n",
        "                w_sum = 0\n",
        "                for l in self.layers:\n",
        "                    w_sum += np.sum(np.square(l.weights))\n",
        "\n",
        "                return w_sum * factor / (2 * m)\n",
        "\n",
        "        raise NameError(\"Regularization method of the type \\\"%s\\\" is not defined!\" % str(method))\n",
        "\n",
        "\n",
        "    def sgd(self, training_data, labels, epochs, learning_rate, mini_batch_size,\n",
        "            reg_method=None, reg_factor=0, verbose=True, gradient_checking=False):\n",
        "        \"\"\"\n",
        "        Fit the network's parameters to the training data using Stochastic Gradient Descent.\n",
        "\n",
        "        :param training_data: numpy ndarray containing, in each row, all the samples of the training set (each row is a\n",
        "        vector of the features representing one of the samples).\n",
        "        :param labels: column vector, in the form of a numpy array of the appropriate shape, containing the labels with\n",
        "        respect to the training set samples.\n",
        "        :param epochs: number of iterations to be ran by the algorithm.\n",
        "        :param mini_batch_size: size of the mini batches of samples to be used in each iteration.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Prep\n",
        "        if gradient_checking:\n",
        "            with open(self.logs_path + \"/log_gradient_checking.txt\", \"w\") as file:\n",
        "                file.write(\"< Started at: \" + str(datetime.datetime.now()) + \" >\\n\\n\")\n",
        "        starting_time = time.time()\n",
        "        all_costs = []\n",
        "\n",
        "        # SGD\n",
        "        for e in range(epochs):\n",
        "            mini_batches = self.generate_mini_batches(training_data, labels, mini_batch_size)\n",
        "            predictions = []\n",
        "\n",
        "            for mini_batch_x, mini_batch_y in zip(mini_batches[0], mini_batches[1]):\n",
        "\n",
        "                # gradient of the cost function (considering the current batch) with respect to the weights and bias\n",
        "                grad_w = [np.zeros(l.weights.shape) for l in self.layers[1:]]\n",
        "                grad_b = [np.zeros(l.bias.shape) for l in self.layers[1:]]\n",
        "                for x, y in zip(mini_batch_x, mini_batch_y):\n",
        "                    x, y = self.colvector(x), self.colvector(y)\n",
        "                    grad_w_variation, grad_b_variation, h = self.backpropagation(x, y)\n",
        "                    predictions.append((h, y))  # save the values of the predictions and its associated label\n",
        "\n",
        "                    grad_w = [cur + var for cur, var in zip(grad_w, grad_w_variation)]\n",
        "                    grad_b = [cur + var for cur, var in zip(grad_b, grad_b_variation)]\n",
        "\n",
        "                m = len(mini_batch_y)\n",
        "                grad_w = [w/m for w in grad_w]\n",
        "                grad_b = [b/m for b in grad_b]\n",
        "\n",
        "                # gradient checking (this won't work if you grad check after updating the params!)\n",
        "                if gradient_checking:\n",
        "                    self.gradient_checking(grad_w, grad_b, mini_batch_x, mini_batch_y)\n",
        "\n",
        "                # updating weights and bias\n",
        "                for i, l in enumerate(self.layers[1:]):\n",
        "                    gw, gb = grad_w[i], grad_b[i]\n",
        "\n",
        "                    l.bias -= (learning_rate * gb)\n",
        "                    reg_term = 0 if reg_method is None else self.regularization_term(m, reg_method,\n",
        "                                                                    reg_factor, derivative=True, w=l.weights)\n",
        "                    l.weights -= learning_rate * (gw + reg_term)\n",
        "\n",
        "            # cost function history\n",
        "            all_costs.append(self.costfunc(predictions))\n",
        "\n",
        "            # print status\n",
        "            if verbose:\n",
        "                pc = (e + 1) / epochs\n",
        "                bar = \"|\" + \"#\" * int(70 * pc) + \"_\" * int(70 * (1 - pc)) + \"|\"\n",
        "\n",
        "                elapsed_time = time.time() - starting_time\n",
        "                remaining_time = elapsed_time * (epochs - e + 1) / (e + 1)\n",
        "                m, s = divmod(remaining_time, 60)\n",
        "                h, m = divmod(m, 60)\n",
        "\n",
        "                try:\n",
        "                    # noinspection PyUnresolvedReferences\n",
        "                    from IPython.display import clear_output  # required to clean the output when using notebooks\n",
        "                    clear_output(wait=True)\n",
        "                except ImportError:\n",
        "                    print(\"\\n\" * 50)\n",
        "\n",
        "                print(\"Epoch: %d/%d  %s  ETA: %02dh %02dmin %02ds\"\n",
        "                      \"\\nTotal loss/cost: %.6f\"\n",
        "                      % ((e + 1), epochs, bar, h, m, s, all_costs[-1]))\n",
        "\n",
        "        return all_costs\n",
        "\n",
        "\n",
        "    def save_params(self, file_name=\"./data/params.txt\"):\n",
        "        \"\"\"\n",
        "        :param file_name:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        file = open(file_name, \"w\")\n",
        "\n",
        "        for i in range(1, len(self.layers)):\n",
        "            l = self.layers[i]\n",
        "            for W in l.weights:\n",
        "                for w in W:\n",
        "                    to_write = str(w).replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "                    file.write(to_write + \" \")\n",
        "                file.write(\"\\n\")\n",
        "\n",
        "            for b in l.bias:\n",
        "                to_write = str(b).replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "                file.write(to_write + \" \")\n",
        "            file.write(\"\\n\")\n",
        "\n",
        "        file.close()\n",
        "\n",
        "\n",
        "    def load_params(self, file_name=\"./data/params.txt\"):\n",
        "        \"\"\"\n",
        "        :param file_name:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        file = open(file_name, \"r\")\n",
        "        for l in self.layers[1:]:\n",
        "\n",
        "            for i in range(len(l.weights)):\n",
        "                raw = file.readline().split()\n",
        "                for j in range(len(l.weights[i])):\n",
        "                    l.weights[i][j] = float(raw[j])\n",
        "\n",
        "            raw = file.readline().split()\n",
        "            for i in range(len(l.bias)):\n",
        "                l.bias[i] = float(raw[i])\n",
        "        file.close()\n",
        "\n",
        "\n",
        "\n",
        "class NeuralLayer:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, input_count, activation=\"sigmoid\"):\n",
        "        self.size = size\n",
        "        self.input_count = input_count\n",
        "        self.activation = activation\n",
        "\n",
        "        if activation.lower() == \"input_layer\":\n",
        "            self.weights, self.bias = None, None\n",
        "        else:\n",
        "            self.weights = np.random.uniform(low=-1, high=1, size=(size, input_count))\n",
        "            self.bias = np.random.uniform(low=-1, high=1, size=(size, 1))\n",
        "\n",
        "\n",
        "    def activate(self, z, derivative=False):\n",
        "        \"\"\"\n",
        "        :param z:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if self.activation.lower() == \"input_layer\":\n",
        "            raise ValueError(\"Tried to activate the neurons from the input layer!\")\n",
        "\n",
        "        if self.activation.lower() == \"sigmoid\":\n",
        "            return 1 / (1 + np.exp(-z)) if not derivative else (self.activate(z) * (1 - self.activate(z)))\n",
        "\n",
        "        if self.activation.lower() == \"relu\":\n",
        "            return np.maximum(z, 0) if not derivative else np.ceil(np.clip(z, 0, 1))\n",
        "\n",
        "        if self.activation.lower() == \"linear\":\n",
        "            return z if not derivative else 1\n",
        "\n",
        "        raise NameError(\"Activation function of the type \\\"%s\\\" is not defined!\" % str(self.activation))\n",
        "\n",
        "\n",
        "\n",
        "def gradient_checking_sample():\n",
        "    \"\"\"\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    m = 32\n",
        "    net = NeuralNetwork([1, 10, 10, 10, 1], layers_activation=\"sigmoid\")\n",
        "    net.layers[-1].activation = \"linear\"\n",
        "\n",
        "    training_data = np.array([x for x in np.random.rand(m)*100])\n",
        "    training_labels = np.array([2*x for x in training_data])\n",
        "\n",
        "    # BACKPROP\n",
        "    bp_grad_w = [np.zeros(l.weights.shape) for l in net.layers[1:]]\n",
        "    bp_grad_b = [np.zeros(l.bias.shape) for l in net.layers[1:]]\n",
        "    for x, y in zip(training_data, training_labels):\n",
        "        x, y = net.colvector(x), net.colvector(y)\n",
        "        gw_var, gb_var, h = net.backpropagation(x, y)\n",
        "\n",
        "        bp_grad_w = [cur + var for cur, var in zip(bp_grad_w, gw_var)]\n",
        "        bp_grad_b = [cur + var for cur, var in zip(bp_grad_b, gb_var)]\n",
        "\n",
        "    bp_grad_w = [w / m for w in bp_grad_w]\n",
        "    bp_grad_b = [b / m for b in bp_grad_b]\n",
        "    bp_grad = net.params_to_row(bp_grad_w, bp_grad_b)\n",
        "\n",
        "    # NUM GRAD\n",
        "    num_grad = net.compute_gradient_numerically(training_data, training_labels)\n",
        "\n",
        "    # COMPARE\n",
        "    for bp, num in zip(bp_grad, num_grad):\n",
        "        print(\"%f  |  %f\" % (bp, num))\n",
        "\n",
        "    relative_error = np.linalg.norm(bp_grad - num_grad) / np.linalg.norm(bp_grad + num_grad)\n",
        "    print(\"\\nRelative error: %.2e\" % relative_error)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXQCtK214uf6",
        "colab_type": "text"
      },
      "source": [
        "**< DIGIT CLASSIFIER >**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYrk6A384xdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def vectorize_labels(labels, vector_size):\n",
        "    \"\"\"\n",
        "\n",
        "    :param y:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    m = len(labels)\n",
        "    vectorized = np.zeros((m, vector_size))\n",
        "\n",
        "    if vector_size > 1:\n",
        "        vectorized = np.zeros((m, vector_size))\n",
        "\n",
        "        for i in range(0, m):\n",
        "            v = np.zeros(vector_size)\n",
        "            v[labels[i]] = 1\n",
        "            vectorized[i] = v\n",
        "    else:\n",
        "        labels.shape = (m, 1)\n",
        "        vectorized = labels\n",
        "\n",
        "    return vectorized\n",
        "\n",
        "\n",
        "def visualize_image(image_array, width, height):\n",
        "    \"\"\"\n",
        "\n",
        "    :param image_array:\n",
        "    :param width:\n",
        "    :param height:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    %matplotlib inline\n",
        "    \n",
        "    img_array = image_array[:]\n",
        "    img_array.shape = (width, height)\n",
        "    \n",
        "    plt.imshow(img_array, cmap='gray', vmin=0, vmax=255)\n",
        "\n",
        "\n",
        "def validate(net, test_set, test_set_labels):\n",
        "    \"\"\"\n",
        "\n",
        "    :param net\n",
        "    :param test_set:\n",
        "    :param test_set_labels:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    hits = 0\n",
        "    for x, y in zip(test_set, test_set_labels):\n",
        "        h = np.argmax(net.predict(x))\n",
        "        hits += 1 if h == y else 0\n",
        "\n",
        "    return hits / len(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "quAK2lSzq_sx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "130d083f-385b-4304-f4a2-e2dbda7b0fe0"
      },
      "source": [
        "# TRAINING\n",
        "data = pd.read_csv(\"./sample_data/mnist_train_small.csv\")\n",
        "training_data, training_labels = data.iloc[:, 1:].values, vectorize_labels(data.iloc[:, 0].values, 10)\n",
        "\n",
        "net = NeuralNetwork([784, 32, 32, 10], logs_path=\"./\")\n",
        "epochs = 500\n",
        "costs = net.sgd(training_data, training_labels, epochs, learning_rate=0.1, \n",
        "        mini_batch_size=32, gradient_checking=False)\n",
        "net.save_params(\"./params_digits_classifier.npz\")\n",
        "\n",
        "# VALIDATING\n",
        "data2 = pd.read_csv(\"./sample_data/mnist_test.csv\")\n",
        "test_data, test_labels = data2.iloc[:, 1:].values, data2.iloc[:, 0].values\n",
        "\n",
        "accuracy = validate(net, test_data, test_labels)\n",
        "print(\"\\nAccuracy: %.2f%%\\n\" % (accuracy*100))\n",
        "\n",
        "# plotting cost function\n",
        "%matplotlib inline\n",
        "plt.plot(range(1, epochs+1), costs)\n",
        "plt.xlabel(\"Epochs\", fontsize=16, color=\"red\")\n",
        "plt.ylabel(\"Cost\", fontsize=16, color=\"red\")\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 500/500  |######################################################################|  ETA: 00h 00min 09s\n",
            "Total loss/cost: 0.054948\n",
            "\n",
            "Accuracy: 92.10%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEPCAYAAABcA4N7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSEAKChERCej/A6cPtvbJafOFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrrkjmT-JBUE",
        "colab_type": "code",
        "outputId": "eeb56f1b-270a-4b24-884a-8bba08e1ddd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# VISUAL TESTING\n",
        "import random\n",
        "\n",
        "index = random.randint(0, len(test_data) - 1)\n",
        "x, y = test_data[index], test_labels[index]\n",
        "visualize_image(x, 28, 28)\n",
        "\n",
        "np.seterr(all=\"ignore\")\n",
        "h = np.argmax(net.predict(x))\n",
        "np.seterr(all=\"warn\")\n",
        "\n",
        "print(\"\\n\\n   Prediction: %d  |  Label: %d\\n\\n\" % (h, y))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "   Prediction: 3  |  Label: 3\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9b+y25suEJsC+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y_c1tFTNuNj",
        "colab_type": "text"
      },
      "source": [
        "**< HOUSE PRICING >**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NQx9FV2N4ZT",
        "colab_type": "code",
        "outputId": "93fe8350-c7b7-4271-f295-c41c9199b966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv(\"./sample_data/california_housing_train.csv\")\n",
        "training_data, training_labels = data.drop(\"median_house_value\", 1).values, data[\"median_house_value\"].values\n",
        "\n",
        "# normalizing the training data\n",
        "mean, std = np.mean(training_data), np.std(training_data)\n",
        "#training_data = (training_data - mean) / std\n",
        "\n",
        "# training the network\n",
        "net = NeuralNetwork(layers_size=[8, 32, 32, 1], layers_activation=\"sigmoid\")\n",
        "net.layers[-1].activation = \"linear\"\n",
        "net.sgd(training_data, training_labels, epochs=100, learning_rate=0.01, \n",
        "        mini_batch_size=32, reg_method=\"l2\", reg_factor=0.1)\n",
        "\n",
        "# validating\n",
        "data2 = pd.read_csv(\"./sample_data/california_housing_test.csv\")\n",
        "testing_data, testing_labels = data2.drop(\"median_house_value\", 1).values, data2[\"median_house_value\"].values\n",
        "#testing_data = (testing_data - mean) / std\n",
        "\n",
        "accepted_error_pc = 0.12\n",
        "hits = 0\n",
        "errors = []\n",
        "\n",
        "for x, y in zip(testing_data, testing_labels):\n",
        "    h = net.predict(x)\n",
        "    errors.append(abs(h - y))\n",
        "    if y*(1 + accepted_error_pc) >= h >= y*(1 - accepted_error_pc):\n",
        "        hits += 1\n",
        "        \n",
        "print(\"Accuracy (considering an error up to %.2f%%): %.2f%%\" % (accepted_error_pc*100, (hits/len(testing_data))*100))\n",
        "print(\"Mean error: %.2f  |  Errors std: %.2f\" % (sum(errors)/len(testing_data), np.std(errors)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100/100  |######################################################################|  ETA: 00h 00min 03s\n",
            "Total loss/cost: 6744803229.791312\n",
            "Accuracy (considering an error up to 12.00%): 15.03%\n",
            "Mean error: 89730.23  |  Errors std: 68865.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBswPiiqTnde",
        "colab_type": "text"
      },
      "source": [
        "**< ESSAY SCORING >**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVjeWOWKD2jV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG6N-htcTqoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def vectorize_labels(labels, size):\n",
        "    grade_interval = 1000 / (size - 1)\n",
        "    m = len(labels)\n",
        "    vectorized = np.zeros((m, size))\n",
        "    \n",
        "    for i in range(0, m):\n",
        "        y = labels[i]\n",
        "        index = int(y / grade_interval)\n",
        "        vectorized[i][index] = 1\n",
        "    \n",
        "    return vectorized\n",
        "        \n",
        "\n",
        "# preparing data\n",
        "root_path = 'gdrive/My Drive/ml_data' \n",
        "data = pd.read_excel(root_path + \"/essays.xlsx\", usecols=\"Z, AA:AM\")\n",
        "data = data[data[\"Nota\"] != 0]\n",
        "\n",
        "data_train = data.sample(frac=0.8)\n",
        "data_test = data.drop(data_train.index)\n",
        "\n",
        "training_data, training_labels = data_train.drop(\"Nota\", 1).values, data_train[\"Nota\"].values\n",
        "testing_data, testing_labels = data_test.drop(\"Nota\", 1).values, data_test[\"Nota\"].values\n",
        "\n",
        "# normalization\n",
        "mean, std = np.mean(training_data), np.std(training_data)\n",
        "training_data = (training_data - mean) / std\n",
        "testing_data = (testing_data - mean) / std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHVkASdNcUud",
        "colab_type": "code",
        "outputId": "0c84c7a9-b2a2-4da4-cc15-53037c4b6384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# TREATING SCORING AS A CLASSIFICATION PROBLEM\n",
        "\n",
        "num_classes = 10\n",
        "v_training_labels = vectorize_labels(training_labels, num_classes)\n",
        "\n",
        "net = NeuralNetwork([13, 16, num_classes], layers_activation=\"sigmoid\")\n",
        "net.sgd(training_data, v_training_labels, epochs=1000, learning_rate=0.01, \n",
        "        mini_batch_size=32, reg_method=\"l2\", reg_factor=0.001)\n",
        "\n",
        "# validating\n",
        "hits = 0\n",
        "for x, y in zip(testing_data, testing_labels):\n",
        "    grade_interval = 1000 / num_classes\n",
        "    y_class = int(y / grade_interval)\n",
        "    \n",
        "    h_class = np.argmax(net.predict(x))\n",
        "    hits += 1 if h_class == y_class else 0\n",
        "    \n",
        "print(\"Accuracy: %.2f%%\" % ( 100*hits / len(testing_data) ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1000/1000  |######################################################################|  ETA: 00h 00min 00s\n",
            "Total loss/cost: 0.381499\n",
            "Accuracy: 20.10%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQu9ZA3qkyNI",
        "colab_type": "code",
        "outputId": "bed1b45a-f30a-4160-fb6b-cec0b69b0fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# TREATING SCORING AS A REGRESSION PROBLEM\n",
        "\n",
        "net = NeuralNetwork([13, 32, 32, 32, 1], layers_activation=\"sigmoid\")\n",
        "net.layers[-1].activation = \"linear\"\n",
        "net.sgd(training_data, training_labels, epochs=10000, learning_rate=0.00005, \n",
        "        mini_batch_size=32, reg_method=\"l2\", reg_factor=3)\n",
        "net.save_params(root_path + \"/essay_regression_params.npz\")\n",
        "\n",
        "# validating\n",
        "accepted_error = 100\n",
        "errors = []\n",
        "hits = 0\n",
        "\n",
        "for x, y in zip(testing_data, testing_labels):\n",
        "    h = net.predict(x)\n",
        "    errors.append(abs(h - y))\n",
        "    hits += 1 if errors[-1] <= accepted_error else 0\n",
        "    \n",
        "print(\"Accuracy (considering an error up to %.2f points): %.2f%%\" % ( accepted_error, 100*hits / len(testing_data) ))\n",
        "print(\"Mean error: %.2f  |  Errors std: %.2f\" % ( sum(errors) / len(testing_data), np.std(errors) ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10000/10000  |######################################################################|  ETA: 00h 00min 00s\n",
            "Total loss/cost: 4528.718352\n",
            "Accuracy (considering an error up to 100.00 points): 67.46%\n",
            "Mean error: 85.44  |  Errors std: 66.60\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}